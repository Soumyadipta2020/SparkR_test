{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# docker run -it --name sparkr-jupy --mount type=bind,source=C:/Users/soumy/OneDrive/Coding,target=/app/data --rm -p 8888:8888 quay.io/jupyter/all-spark-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘SparkR’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(SparkR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "# Creating spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in sparkR.session():\n",
      "“SparkR is deprecated from Apache Spark 4.0.0 and will be removed in a future version.”\n",
      "Spark package found in SPARK_HOME: /usr/local/spark\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching java with spark-submit command /usr/local/spark/bin/spark-submit   sparkr-shell /tmp/RtmpDVW8AT/backend_portc746b55fc4c \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.SparkSession id 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in sparkR.session(master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"2g\")):\n",
      "“SparkR is deprecated from Apache Spark 4.0.0 and will be removed in a future version.”\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.SparkSession id 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sparkR.session()\n",
    "if (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) {\n",
    "  Sys.setenv(SPARK_HOME = \"/home/spark\")\n",
    "}\n",
    "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n",
    "sparkR.session(master = \"local[*]\",\n",
    "               sparkConfig = list(spark.driver.memory = \"2g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+--------+----+\n",
      "|      date|   open|   high|    low|  close|  volume|Name|\n",
      "+----------+-------+-------+-------+-------+--------+----+\n",
      "|2014-05-05|   74.5|  74.66|  73.76|  74.51| 4348608| BAX|\n",
      "|2016-05-26| 212.66|214.115| 210.23| 212.05|  422492| ADS|\n",
      "|2015-06-17| 139.75|140.148| 139.08| 139.76|  650779| BDX|\n",
      "|2015-09-21| 135.33| 136.12| 133.86| 134.87| 1059194| APD|\n",
      "|2013-11-19|74.1471|74.7685|73.9956|74.2214|52234707|AAPL|\n",
      "+----------+-------+-------+-------+-------+--------+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_raw <- loadDF(\n",
    "  path = \"/app/data/R/SparkR_test/SparkR_test/all_stocks_5yr.csv\", # File path\n",
    "  source = \"csv\", # file type\n",
    "  header = \"true\", # data have header or not\n",
    "  inferSchema = \"true\" # auto column type prediction\n",
    ")\n",
    "df <- repartition(df_raw, 10) # partition the data\n",
    "\n",
    "showDF(df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date|  open|\n",
      "+----------+------+\n",
      "|2016-12-22|123.72|\n",
      "|2013-12-02| 23.55|\n",
      "|2014-08-06|149.21|\n",
      "|2017-07-26| 129.0|\n",
      "|2016-08-16| 72.63|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "temp <- select(df, # Dataframe\n",
    "  c(\"date\", \"open\") # Column names to select\n",
    ")\n",
    "\n",
    "showDF(temp, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+------+------+-------+----+\n",
      "|      date|  open|   high|   low| close| volume|Name|\n",
      "+----------+------+-------+------+------+-------+----+\n",
      "|2014-05-05|  74.5|  74.66| 73.76| 74.51|4348608| BAX|\n",
      "|2016-05-26|212.66|214.115|210.23|212.05| 422492| ADS|\n",
      "|2015-06-17|139.75|140.148|139.08|139.76| 650779| BDX|\n",
      "+----------+------+-------+------+------+-------+----+\n"
     ]
    }
   ],
   "source": [
    "# Select first 3 rows\n",
    "temp <- limit(df, 3)\n",
    "\n",
    "showDF(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+-------+------+-------+----+\n",
      "|      date|   open|  high|    low| close| volume|Name|\n",
      "+----------+-------+------+-------+------+-------+----+\n",
      "|2016-03-22| 100.08|100.37| 99.455| 99.93|1435782| AMT|\n",
      "|2014-12-10| 100.22| 101.1|  99.97|100.17|1826854| AMT|\n",
      "|2014-12-08|  104.0|104.17|99.6556|100.71|3484193|AVGO|\n",
      "|2017-11-02| 100.96|102.45|  99.27|100.21| 630470| AIZ|\n",
      "|2017-12-21|100.505|101.45|  99.82|101.12| 393034| AIZ|\n",
      "+----------+-------+------+-------+------+-------+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "temp <- filter(df, df$open > 100 & df$low < 100)\n",
    "\n",
    "showDF(temp, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType\n",
       "|-name = \"date\", type = \"DateType\", nullable = TRUE\n",
       "|-name = \"open\", type = \"DoubleType\", nullable = TRUE\n",
       "|-name = \"high\", type = \"DoubleType\", nullable = TRUE\n",
       "|-name = \"low\", type = \"DoubleType\", nullable = TRUE\n",
       "|-name = \"close\", type = \"DoubleType\", nullable = TRUE\n",
       "|-name = \"volume\", type = \"IntegerType\", nullable = TRUE\n",
       "|-name = \"Name\", type = \"StringType\", nullable = TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+---------+\n",
      "|      date|Name|Metric|   Values|\n",
      "+----------+----+------+---------+\n",
      "|2014-05-05| BAX|  open|     74.5|\n",
      "|2014-05-05| BAX|  high|    74.66|\n",
      "|2014-05-05| BAX|   low|    73.76|\n",
      "|2014-05-05| BAX| close|    74.51|\n",
      "|2014-05-05| BAX|volume|4348608.0|\n",
      "+----------+----+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Pivot Longer function\n",
    "pivot_longer_sparkr <- function(df, id_cols, name_col, value_col) {\n",
    "  library(SparkR)\n",
    "  # Get all column names from the DataFrame\n",
    "  all_cols <- colnames(df)\n",
    "  # Identify the columns to pivot by excluding the specified columns\n",
    "  cols_to_longer <- setdiff(all_cols, id_cols)\n",
    "  # Use stack to pivot the columns\n",
    "  long_df <- unpivot(df, id_cols, cols_to_longer, name_col, value_col)\n",
    "  return(long_df)\n",
    "}\n",
    "\n",
    "# Applying the function\n",
    "temp <- pivot_longer_sparkr(df, id_cols = c(\"date\", \"Name\"),\n",
    "                            name_col = \"Metric\", value_col = \"Values\")\n",
    "\n",
    "showDF(temp, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
